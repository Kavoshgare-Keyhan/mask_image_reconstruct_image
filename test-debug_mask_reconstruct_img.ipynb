{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f01a722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fayyazct/anaconda3/envs/vqvae-transformer/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[neptune] [warning] NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse, math, sys, os, random\n",
    "import torch\n",
    "from torch import nn\n",
    "import distributed as dist\n",
    "from transformers import DistilBertForMaskedLM, DistilBertConfig\n",
    "from vqvae import FlatVQVAE\n",
    "import neptune.new as neptune\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "os.nice(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4587c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath='/home/fayyazct/attention/mask_image_reconstruct_image'\n",
    "#path='/home/abghamtm/work/masking_comparison/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cdbd1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fayyazct/attention/mask_image_reconstruct_image/checkpoint/vqvae/model_epoch80_flat_vqvae80x80_144x456codebook.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pathfinal=mypath+\"/checkpoint/vqvae/model_epoch80_flat_vqvae80x80_144x456codebook.pth\"\n",
    "print(pathfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4f626ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_mask(quantizes, indices, n_token, mask_perc=1):\n",
    "    mask_pattern = np.random.default_rng().choice([True, False], size=(1,1, n_token), p=[mask_perc, 0])\n",
    "    mask_quantizes = quantizes.clone() # shallow copy\n",
    "    mask_quantizes[mask_pattern] = 0  # Assuming 0 is the mask token\n",
    "    mask_indices = indices.clone()\n",
    "    mask_indices[~mask_pattern[0]] = -100 # Assuming -100 is the mask label token\n",
    "    return mask_quantizes, mask_indices, mask_pattern\n",
    "\n",
    "def custom_mask(quantizes, indices, n_token, mask_pattern, unmask_pattern):\n",
    "    if unmask_pattern: mask_pattern[0,0,unmask_pattern] = False\n",
    "    mask_quantizes = quantizes.clone() # shallow copy\n",
    "    mask_quantizes[mask_pattern] = 0  # Assuming 0 is the mask token\n",
    "    mask_indices = indices.clone()\n",
    "    mask_indices[~mask_pattern[0]] = -100 # Assuming -100 is the mask label token\n",
    "    return mask_quantizes, mask_indices, mask_pattern\n",
    "\n",
    "def random_mask(quantizes, indices, n_sample, n_token, mask_perc):\n",
    "    mask_pattern = np.random.default_rng().choice([True, False], size=(1,1, n_token), p=[mask_perc, 1 - mask_perc])\n",
    "    mask_quantizes = quantizes.clone()\n",
    "    mask_quantizes[mask_pattern] = 0  # Assuming 0 is the mask token\n",
    "    mask_indices = indices.clone()\n",
    "    mask_indices[~mask_pattern[0]] = -100 # Assuming -100 is the mask label token\n",
    "    return mask_quantizes, mask_indices, mask_pattern[0][0]\n",
    "\n",
    "def selective_masking(distil, quantized, indices, mask_percentage):\n",
    "    total_num = quantized.shape[1] \n",
    "    total_unmasked_number = (int) (total_num * (1-mask_percentage))\n",
    "    unmask_index = (int) (total_num/2)\n",
    "    quantized_masked = torch.zeros_like(quantized)\n",
    "    mask_pattern = torch.ones(quantized.shape[:2], dtype=torch.bool)\n",
    "    already_unmasked = set()\n",
    "    for i in range(0,total_unmasked_number):\n",
    "        mask_pattern[0,unmask_index] = False\n",
    "        already_unmasked.add(unmask_index)\n",
    "        quantized_masked[0, unmask_index] = quantized[0,unmask_index]\n",
    "        outputs = distil(inputs_embeds = quantized_masked, output_hidden_states = True)\n",
    "        max_logits, max_indices = torch.max(outputs.logits, dim=-1)\n",
    "        sorted_logits, sorted_indices = torch.sort(max_logits[0])\n",
    "        for min_index in sorted_indices:\n",
    "            if min_index.item() not in already_unmasked:\n",
    "                unmask_index = min_index.item()\n",
    "                break\n",
    "    indices_masked = indices.clone()\n",
    "    indices_masked[~mask_pattern[0]] = -100\n",
    "    return quantized_masked, indices_masked, mask_pattern[0]\n",
    "\n",
    "\n",
    "def selective_mask_update(distil, quantized, indices, d_embed_vec, n_token=400):\n",
    "    mask_pattern = torch.zeros(quantized.shape[:2])\n",
    "    outputs = distil(inputs_embeds = quantized_masked, output_hidden_states = True)\n",
    "    max_logits, max_indices = torch.max(outputs.logits, dim=-1)\n",
    "    sorted_logits, sorted_indices = torch.sort(max_logits[0])\n",
    "    sort_np = sorted_indices.cpu().numpy()\n",
    "    return sort_np\n",
    "\n",
    "\n",
    "def load_embedding_space(path=mypath+'/checkpoint/vqvae/quantized_epoch80_flat_vqvae80x80_144x456codebook.npy'):\n",
    "    quantizes = np.load(path)\n",
    "    quant_b = quantizes\n",
    "    n, c, h, w = quantizes.shape\n",
    "    quantizes = quantizes.transpose(0, 2, 3, 1)\n",
    "    quantizes = quantizes.reshape(n, h * w, c)\n",
    "    return quantizes, quant_b\n",
    "\n",
    "def load_indices(path=mypath+'/checkpoint/vqvae/indices_epoch80_flat_vqvae80x80_144x456codebook.npy'):\n",
    "    indices = np.load(path)\n",
    "    n, h, w = indices.shape\n",
    "    indices = indices.reshape(n, h * w)\n",
    "    return indices\n",
    "\n",
    "def load_labels(path=mypath+'/checkpoint/vqvae/labels_epoch80_flat_vqvae80x80_144x456codebook.npy'):\n",
    "    labels = np.load(path)\n",
    "    # labels = torch.from_numpy(labels)\n",
    "    return labels\n",
    "\n",
    "def vqvae_model_setup(ckpt_vqvae, device):\n",
    "    model_vqvae = FlatVQVAE().to(device)\n",
    "    model_vqvae.load_state_dict(torch.load(ckpt_vqvae, map_location=device))\n",
    "    model_vqvae = model_vqvae.to(device)\n",
    "    model_vqvae.eval()\n",
    "    return model_vqvae\n",
    "    \n",
    "def setup_resources():\n",
    "    device = torch.device('cpu')\n",
    "    #torch.cuda.set_device(3)\n",
    "    #torch.cuda.empty_cache()\n",
    "    #if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    #     device = [f\"cuda:{i}\" for i in range(torch.cuda.device_count())] # need to becustomized to select not running GPUs\n",
    "    #else:\n",
    "    #    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    return device\n",
    "\n",
    "def transformer_setup(device, model_pth, n_token, vocab_size, d_embed_vec):\n",
    "    cfg = DistilBertConfig(\n",
    "            vocab_size = vocab_size,\n",
    "            hidden_size = d_embed_vec,\n",
    "            sinusoidal_pos_embds = False,\n",
    "            n_layers = 6,\n",
    "            n_heads = 4,\n",
    "            max_position_embeddings = n_token\n",
    "    )\n",
    "    model_distil = DistilBertForMaskedLM(cfg).to(device)\n",
    "    model_distil.load_state_dict(torch.load(model_pth))\n",
    "    model_distil = model_distil.to(device)\n",
    "    model_distil.eval()\n",
    "    return model_distil\n",
    "\n",
    "def vqvae_setup(device, model_pth):\n",
    "    model_vqvae = FlatVQVAE().to(device)\n",
    "    model_vqvae.load_state_dict(torch.load(model_pth, map_location=device))\n",
    "    model_vqvae = model_vqvae.to(device)\n",
    "    model_vqvae.eval()\n",
    "    return model_vqvae\n",
    "\n",
    "def mask_iter(min, max, step, n_token, arbitrary_percentage:list =[.85,.95]):\n",
    "    mask_percentages = np.arange(min, max, step)\n",
    "    mask_percentages = np.append(mask_percentages, arbitrary_percentage) if arbitrary_percentage else mask_percentages\n",
    "    mask_percentages = np.sort(mask_percentages)\n",
    "    reverse_mask_percentages = mask_percentages[::-1] # reverse the sorted array\n",
    "    mask_perc_map_indices_length = n_token * reverse_mask_percentages\n",
    "    return mask_percentages, reverse_mask_percentages, mask_perc_map_indices_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a7c3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = setup_resources()\n",
    "quantizes, quant_b = load_embedding_space()\n",
    "indices = load_indices()\n",
    "labels = load_labels()\n",
    "\n",
    "n_sample = quantizes.shape[0]\n",
    "d_embed_vec = quantizes.shape[2]\n",
    "n_token = np.prod(quantizes.shape[1])\n",
    "quantizes = quantizes.reshape((n_sample, n_token, d_embed_vec))\n",
    "length = int(math.sqrt(n_token))\n",
    "indices = indices.reshape((n_sample, n_token))\n",
    "indices_to_sort = set(indices.flatten())\n",
    "indices_to_sort = sorted(indices_to_sort)\n",
    "vocab_size = indices_to_sort[-1] + 1\n",
    "\n",
    "model_distil = transformer_setup(device=device, n_token=n_token, vocab_size=vocab_size, d_embed_vec=d_embed_vec, model_pth=mypath+\"/checkpoint/distil/80x80_100ClassImagenet_flat_144x456codebook_75mask_epoch100.pt\")\n",
    "model_vqvae = vqvae_setup(device=device, model_pth=mypath+\"/checkpoint/vqvae/model_epoch80_flat_vqvae80x80_144x456codebook.pth\")\n",
    "mask_percentages, reverse_mask_percentages, mask_perc_map_indices_length = mask_iter(min=0.1, max=1.1, step=0.1, n_token=n_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d418f077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 144)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantizes[0][[4,7,2]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73b981cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_np(img_np, save_path):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    np.save(save_path, img_np)\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "# Loop through image list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d39807ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additive approach - event: 0\n",
      "step 0 for mask percentage 1.0 is done\n",
      "Saved: /home/fayyazct/attention/mask_image_reconstruct_image/image/recons/additive_img_data/00000_TrueLabel=561_MaskPercentage=100_AdditiveMaskingWithTransformer.npy\n",
      "code reaches here\n",
      "code reaches inside try block and its first loop\n",
      "code reaches inside try block and its second loop\n",
      "code reaches inside try block and its second loop\n",
      "code reaches inside try block and its second loop\n",
      "code reaches inside try block and its second loop\n",
      "code reaches inside try block and its second loop\n",
      "code successfully filled unmask indices and exit the 2nd loop\n",
      "Unmask indices: [362, 342, 341, 361, 363]\n"
     ]
    }
   ],
   "source": [
    "reconstruction_error = np.zeros((n_sample, len(reverse_mask_percentages)))\n",
    "criterion = nn.MSELoss()\n",
    "x = 0\n",
    "\n",
    "q = torch.from_numpy(quantizes[x]).to(device)\n",
    "q = torch.reshape(q, (1, q.size(dim=0), q.size(dim=1)))\n",
    "index = torch.from_numpy(indices[x]).to(device)\n",
    "label = labels[x]\n",
    "\n",
    "'''\n",
    "Outer increments: 12\n",
    "Inner increments: 4+4+4+4+8+8+8+8+8+8+8 = 72\n",
    "'''\n",
    "#do it every 10 percent\n",
    "n_iter = 0\n",
    "for i in range(len(mask_perc_map_indices_length)):\n",
    "    n_iter += 1  # for the outer loop\n",
    "    if i < len(mask_perc_map_indices_length) - 1:\n",
    "        n_iter += int((mask_perc_map_indices_length[i] - mask_perc_map_indices_length[i+1]) // 5)\n",
    "\n",
    "stack_mask_pattern = np.zeros((n_iter, n_token), dtype=bool)\n",
    "stack_output_logits = np.zeros((n_iter, n_token, vocab_size), dtype=float) # 80 is the number of events, 400 is the number of tokens represents each pixel of encoded latent space which has the shape of 20*20, and 456 is the number of codebook vectors\n",
    "unmask_indices = []\n",
    "i = 0 # iteration on mask_perc_map_indices_length\n",
    "l = 0 # iteration on the number of total iterations corresponding to the number of mask and logits generation (number of calling the transformer model)\n",
    "\n",
    "q_masked, index_masked, mask_pattern = full_mask(q, index, n_token)   \n",
    "q_masked = q_masked.to(device)\n",
    "index_masked = index_masked.to(device)\n",
    "index_masked_for_visual = index.clone()\n",
    "\n",
    "while i < len(mask_perc_map_indices_length):\n",
    "    print('additive approach - event:', x)           \n",
    "    with torch.no_grad():\n",
    "        outputs = model_distil(inputs_embeds = q_masked, output_hidden_states = True)\n",
    "        stack_output_logits[l] = outputs.logits[0].detach().cpu().numpy() # save the logits for each iteration\n",
    "        stack_mask_pattern[l] = mask_pattern[0][0] # save the mask pattlogitsern for each iteration\n",
    "        print(f'step {l} for mask percentage {reverse_mask_percentages[i]} is done')\n",
    "        l += 1\n",
    "        confidence, ind_monst_probable = torch.max(outputs.logits, dim=2) # the first variable is the max_logits, and the second variable is the indices of the max logits\n",
    "        confidence_based_recons_index = ind_monst_probable[0]\n",
    "        confidence_based_recons_index = confidence_based_recons_index.to(device)\n",
    "        # Flatten the tensor to 1D\n",
    "        confidence_flat = confidence.flatten()\n",
    "        # Get indices that would sort the tensor in ascending order\n",
    "        conf_indices_min_max = torch.argsort(confidence_flat, dim=0)\n",
    "    # stack_mask_pattern[i] = mask_pattern[0][0]\n",
    "\n",
    "    if unmask_indices:\n",
    "        confidence_based_recons_index[unmask_indices] = index[unmask_indices]\n",
    "    # print('code reaches here')    \n",
    "    with torch.no_grad():\n",
    "        distil_out = model_vqvae.decode_code(torch.reshape(confidence_based_recons_index, (1, length,length)).to(device))\n",
    "\n",
    "        vqvae_out = model_vqvae.decode(torch.from_numpy(quant_b[x]).to(device)) #torch.reshape(torch.from_numpy(indices[x]), (1,length,length)).to(device)\n",
    "        \n",
    "        index_masked_for_visual[mask_pattern[0][0]] = 0\n",
    "        vqvae_masked_out = model_vqvae.decode_code(torch.reshape(index_masked_for_visual, (1,length,length)).to(device))\n",
    "\n",
    "    percentage = int(reverse_mask_percentages[i]*100)\n",
    "    img_list = [vqvae_out, vqvae_masked_out, distil_out]\n",
    "    # label_list = [vqvae_img_label.item(), vqvae_masked_img_label.item(), add_mask_img_label.item()]\n",
    "    '''\n",
    "    for ii, img in enumerate(img_list):\n",
    "        img_np = img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        if ii==0 and percentage==100:\n",
    "            np.save(f'/image/recons/original_img_reconstrcted_data/{str(x).zfill(5)}_OriginalImage_TrueLabel={label}.npy', img_np)\n",
    "        elif ii==1:\n",
    "            np.save(f'/home/abghamtm/work/masking_comparison/image/recons/aditive_img_data/{str(x).zfill(5)}_MaskPercentage={percentage}_AdditiveMasking_TrueLabel={label}.npy', img_np)\n",
    "        else:\n",
    "            np.save(f'/home/abghamtm/work/masking_comparison/image/recons/aditive_img_data/{str(x).zfill(5)}_MaskPercentage={percentage}_AdditiveMaskingWithTransformer_TrueLabel={label}.npy', img_np)\n",
    "    '''\n",
    "    for ii, img in enumerate(img_list):\n",
    "        img_np = img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        base_name = f\"{str(x).zfill(5)}_TrueLabel={label}\"\n",
    "\n",
    "        if ii == 0 and percentage == 100:\n",
    "            save_path = f\"/home/abghamtm/work/masking_comparison/image/recons/original_img_reconstrcted_data/{base_name}_OriginalImage.npy\"\n",
    "        elif ii == 1:\n",
    "            save_path = f\"/home/abghamtm/work/masking_comparison/image/recons/additive_img_data/{base_name}_MaskPercentage={percentage}_AdditiveMasking.npy\"\n",
    "        else:\n",
    "            save_path = f\"/home/abghamtm/work/masking_comparison/image/recons/additive_img_data/{base_name}_MaskPercentage={percentage}_AdditiveMaskingWithTransformer.npy\"\n",
    "\n",
    "    # Modify save path with mypath\n",
    "    save_path = save_path.replace(\"/home/abghamtm/work/masking_comparison\", mypath)\n",
    "\n",
    "\n",
    "    save_image_np(img_np, save_path)\n",
    "\n",
    "    \n",
    "    recons_loss = criterion(distil_out, vqvae_out.unsqueeze(0))\n",
    "    reconstruction_error[x, i] = recons_loss.item()\n",
    "    print('code reaches here')  \n",
    "    try: # the purpose of this try-except block is to avoid the error when the mask_perc_map_indices_length index i+1 reaches 12\n",
    "        # unmask indices gradually \n",
    "        for j in range(int((mask_perc_map_indices_length[i]-mask_perc_map_indices_length[i+1])//5)):\n",
    "            print('code reaches inside try block and its first loop') \n",
    "            c = 0\n",
    "            for k in range(len(conf_indices_min_max)):\n",
    "                print('code reaches inside try block and its second loop') \n",
    "                if conf_indices_min_max[k].item() not in unmask_indices:\n",
    "                    unmask_indices.append(conf_indices_min_max[k].item())\n",
    "                    c += 1\n",
    "                    if c == 5:\n",
    "                        break\n",
    "            \n",
    "            print('code successfully filled unmask indices and exit the 2nd loop')\n",
    "            print(f'Unmask indices: {unmask_indices}')\n",
    "            q_masked, index_masked, mask_pattern = custom_mask(q, index, n_token, mask_pattern, unmask_indices)\n",
    "            # # check q_masked and mask_pattern completely follow the same pattern\n",
    "            # n_masked_ind = np.count_nonzero(mask_pattern)\n",
    "            # num_zeros = torch.sum(q_masked == 0).item()\n",
    "            if (q_masked[unmask_indices] == 0).all(): # I have to divide num_zeros by q_masked.shape[2] because q_masked is a 3D tensor and we want to check the number of zeros per token/embedded vector\n",
    "                raise ValueError(f\"Unmask codebooks are populated by zeros\")\n",
    "            '''\n",
    "                There are some zeros inside each quantized vector for each image, so when we use custom mask function to unmask some tokens/embedded vectors, there are few zeros left in the qunmasked tokens. So the number of zeros in the q_masked is not equal to the number of masked indices.\n",
    "                example for x=0: according to first unmask indices ([362, 342, 341, 361, 363]), we expect 56880 zeros while it contains 56893 zeros \n",
    "            '''\n",
    "            # print('code is ready to call the transformer model again for updated q_masked')\n",
    "            outputs = model_distil(inputs_embeds = q_masked, output_hidden_states = True)\n",
    "            stack_output_logits[l] = outputs.logits[0].detach().cpu().numpy() # save the logits for each iteration\n",
    "            stack_mask_pattern[l] = mask_pattern[0][0] # save the mask pattern for each iteration\n",
    "            print(f'step {l} for mask percentage {reverse_mask_percentages[i]} is done')\n",
    "            l += 1\n",
    "            confidence, ind_monst_probable = torch.max(outputs.logits, dim=2)\n",
    "            confidence_flat = confidence.flatten()\n",
    "            conf_indices_min_max = torch.argsort(confidence_flat, dim=0)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "    i+=1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9897568",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask_perc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     mask_pattern \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng()\u001b[38;5;241m.\u001b[39mchoice(\n\u001b[1;32m     24\u001b[0m         [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m], size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, n_token), p\u001b[38;5;241m=\u001b[39m[mask_perc, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask_perc]\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     mask_pattern \u001b[38;5;241m=\u001b[39m update_mask(mask_pattern, \u001b[43mmask_perc\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask_perc' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def update_mask(mask_pattern, mask_perc):\n",
    "    # mask_pattern: existing boolean mask (e.g., shape (1, 1, n_token))\n",
    "    # mask_perc: probability to mask unmasked positions\n",
    "\n",
    "    # Find positions that are currently unmasked (False)\n",
    "    unmasked = ~mask_pattern\n",
    "\n",
    "    # Generate new random mask for only the unmasked positions\n",
    "    new_mask = np.random.default_rng().choice(\n",
    "        [True, False],\n",
    "        size=mask_pattern.shape,\n",
    "        p=[mask_perc, 1 - mask_perc]\n",
    "    )\n",
    "\n",
    "    # Only update positions that are currently unmasked\n",
    "    updated_mask = mask_pattern | (unmasked & new_mask)\n",
    "    return updated_mask\n",
    "\n",
    "\n",
    "if mask_pattern is None:\n",
    "    mask_pattern = np.random.default_rng().choice(\n",
    "        [True, False], size=(1, 1, n_token), p=[mask_perc, 1 - mask_perc]\n",
    "    )\n",
    "else:\n",
    "    mask_pattern = update_mask(mask_pattern, mask_perc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c04ee80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unmask_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8f022c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56880"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "395*q_masked.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd7a8b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_zeros/q_masked.shape[2] != n_masked_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "310b39d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stack_mask_pattern[0] == mask_pattern[0][0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a98979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/run/user/12714/jupyter/runtime/kernel-v3217805922cd0008aa7132cb872b52334096aebdf.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "                outputs = model_distil(inputs_embeds = q_masked, output_hidden_states = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
